defaults:
  - default_config
  - lang_rew_module
  - _self_

hydra:
  job:
    chdir: True
    name: "SEED-${CONSTANT.RANDOM_SEED}-Montezuma_RND_rl_env_train_task_id-${rl_data_files.rl_task}_Lang_Rew-${rl_params.want_lang_rew}_Confidence_threshold-${rl_params.lang_softmax_threshold_current}_Mode-${rl_params.lang_rew_mode}"
  run:
    dir: "outputs/${now:%Y-%m-%d}/rl_env_train_${now:%H-%M-%S}"

CONSTANT:
  ENV_NAME: 'MontezumaRevengeNoFrameskip-v4' # BreakoutNoFrameskip-v4 MontezumaRevengeNoFrameskip-v4
  N_ACTIONS: 8 # In the future we will change this to 8
  MAX_STEPS: 1000
  RANDOM_START: 30
  IMG_MEAN: "[0.485 * 255.0, 0.456 * 255.0, 0.406 * 255.0]"
  IMG_STD: "[0.229 * 255.0, 0.224 * 255.0, 0.225 * 255.0]"
  RANDOM_SEED: 1

data_files:
  use_wandb: True

rl_params:
  algo: rnd # Algorithm to use: rnd | ppo
  lr: 1.0e-4
  num_step: 128 # number of forward steps
  eps: 0.1 # TODO may increase this to explore more
  ext_gamma: 0.99
  int_gamma: 0.99
  use_gae: True # 'use generalized advantage estimation (default: True)'
  gae_lambda: 0.95 # Lambda coefficient in GAE formula (default: 0.95, 1 means no gae)
  use_noisy_net: False # use NoisyNet
  sticky_action: True #
  sticky_action_prob: 0.25
  mini_batch: 4
  entropy_coef: 0.001
  ext_coef: 2.0 # this will become the coef for lang rew if lang rew is activated
  int_coef: 1.0
  max_episode_steps: 1200
  pre_obs_norm_steps: 50
  save_interval: 100
  load_model: False
  log_dir: None
  save_dir: "rl_trained_models"
  recurrent_policy: False
  batch_size: 256 # 800 for 25, 4096 for 128, 2048 for 64, 512 for 16 num worker CHANGEABLE batch_size = int(args.num_step * args.num_worker / args.mini_batch)


  num_worker: 8



  # lang rew configs
  window_size: 2 # default 2, 2 x 4 = 8 which is the stride size for training lang rew module
  lang_accu_rew_maximum: 1.0
  max_decay_coef: 1.0 # 0.99999, if do not want to change, then set to 1.0


  fixed_lang_reward: False # False | 0.2 fixed value if true, give fix reward and close the sentence
  lang_softmax_threshold_current: 0.89 # if softmax exceed the value, we activate the next sentence default 0.7
  # 0.6  lang rew mode 2 rl task 0
  # 0.89 lang rew mode 2 rl task 1
  lang_softmax_threshold_old: 0.2 # this is used for event detect threshold
  # 0.2  lang rew mode 5 rl task 2


  minimum_memory_len: 18 # default 18 # 18/3=6, so every 6 x 4 = 24 frame we update the language reward
  memory_length: 36 # default 13 # but the issue is our event is quite short


  want_lang_rew: True # set to False for false positive Simulation
  lang_rew_mode: 5 # 1. binary cls 2. binary cls hard neg  3. goyal (tested) 4. event det  5. event det hard neg
  # looks like binary classification can be used for easy and normal task
  # event det can be used for hard task


rl_data_files:
  rl_task: 2 # 0 easy, 1 normal, 2 hard

  whether_hard_instruction: False # whether we want complex instructions to make the multimodal task easier

  lang_rew_module_dictpath_binary_cls: "saved_model/trained_model/bi_cls-obj_det-T-rel_off-T-act_pred-T-neg_rat-1.0.pth"
  lang_rew_module_dictpath_binary_cls_hard_negative: "saved_model/trained_model/bi_cls-obj_det-T-rel_off-T-act_pred-T-neg_rat-0.7.pth"
  lang_rew_module_dictpath_event_det: "saved_model/trained_model/event_det-obj_det-T-rel_off-T-act_pred-T-neg_rat-1.0.pth"
  lang_rew_module_dictpath_event_det_hard_negative: "saved_model/trained_model/event_det-obj_det-T-rel_off-T-act_pred-T-neg_rat-0.7.pth"
  lang_rew_module_dictpath_goyal: "goyal_original/original_goyal_lang_rew_state_dict.pth"

  easy_task_saved_state: "${data_files.data_path}/testing_data/easy_task/easy_mr_system_state_cross_laser_gate_room_0.pkl"
  normal_task_saved_state: "${data_files.data_path}/testing_data/normal_task/normal_mr_system_state_torch_room_5.pkl"
  hard_task_saved_state: "${data_files.data_path}/testing_data/hard_task/hard_mr_system_state_grab_key_room_1.pkl"

  hard_task_walkthrough_filepath_no_hard: '${data_files.data_path}/testing_data/hard_task/test_text_dict_no_hard.pkl' # key: 'spell_correction_txt', embedding
  hard_task_walkthrough_filepath: '${data_files.data_path}/testing_data/hard_task/test_text_dict.pkl' # key: 'spell_correction_txt', embedding
  hard_task_walkthrough_filepath_goyal: '${data_files.data_path}/testing_data/hard_task/test_text_dict_glove.pkl'# key: embedding, sentence

  normal_task_walkthrough_filepath_no_hard: '${data_files.data_path}/testing_data/normal_task/test_text_dict_no_hard.pkl' # key: 'spell_correction_txt', embedding
  normal_task_walkthrough_filepath: '${data_files.data_path}/testing_data/normal_task/test_text_dict.pkl' # key: 'spell_correction_txt', embedding
  normal_task_walkthrough_filepath_goyal: '${data_files.data_path}/testing_data/normal_task/test_text_dict_glove.pkl'# key: embedding, sentence

  easy_task_walkthrough_filepath_no_hard: '${data_files.data_path}/testing_data/easy_task/test_text_dict_no_hard.pkl' # key: 'spell_correction_txt', embedding
  easy_task_walkthrough_filepath: '${data_files.data_path}/testing_data/easy_task/test_text_dict.pkl' # key: 'spell_correction_txt', embedding
  easy_task_walkthrough_filepath_goyal: '${data_files.data_path}/testing_data/easy_task/test_text_dict_glove.pkl'# key: embedding, sentence


